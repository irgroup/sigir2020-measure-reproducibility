{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Compute RBO</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neede libraries\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from rbo import rbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Input Parameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput Parameters\n",
    "# Run Name: \"WCrobust04\" or \"WCrobust0405\"\n",
    "original_run_name = \"WCrobust04\"\n",
    "\n",
    "# RBO persistency parameter\n",
    "p = 0.8\n",
    "\n",
    "# Path to the folder with the replicated runs\n",
    "replicability_runs_folder = os.path.join(\"/Users/tjz514/Documents/uni/2020/sigir2020_reproducibility/experiments/runs/replicability_rename\", original_run_name.lower())\n",
    "\n",
    "# Path to the folder with the original runs\n",
    "original_run_file = os.path.join(\"/Users/tjz514/Documents/uni/2020/sigir2020_reproducibility/experiments/runs/original\", original_run_name + \"_replicability.txt\")\n",
    "\n",
    "# Output path, where the csv file with RBO scores will be saved\n",
    "output_path = os.path.join(\"/Users/tjz514/Documents/uni/2020/sigir2020_reproducibility/experiments/matlab/results/measures\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Util Functions</h3>\n",
    "<ul>\n",
    "<li><code>import_run</code>: function to import the runs from a text file;</li>\n",
    "<li><code>print_results</code>: function to print RBO scores in a csv file</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_run(run_file):\n",
    "    # Import a run\n",
    "    # Each run is a dictionary (topic_id, ranking), where:\n",
    "    # - key: is the topic_id (string)\n",
    "    # - value: is the list of documents id\n",
    "    # We just need a list of doc_id for each topic\n",
    "    # We sort the documents as in trec_eval\n",
    "    \n",
    "    # Args:\n",
    "        # run_file: path to the file to be imported.\n",
    "\n",
    "    # Returns:\n",
    "        # The script parses each line of the input file and maps it to a dictionary\n",
    "        # run[topic_id] = [per_topic_ranking]\n",
    "\n",
    "        # The script adopts the same ordering as trec_eval, i.e. it sorts rankings \n",
    "        # by descending order of score and descending lexicographical order of \n",
    "        # doc_id. Moreover, as done by trec_eval, it represents the document \n",
    "        # score as a float.\n",
    "\n",
    "    # Initialize the run as a dictionary\n",
    "    run = {}\n",
    "\n",
    "    # Read the input file as a whole\n",
    "    with open(run_file) as f:\n",
    "        input_file = f.readlines()\n",
    "\n",
    "    # Close the run file\n",
    "    f.close()    \n",
    "\n",
    "    # Initialize the current topic id\n",
    "    old_topic_id = \"-\"\n",
    "\n",
    "    # Parse each line and populate the corresponding dictionaries\n",
    "    for line in input_file:\n",
    "        # parse the line and keep the topic_id, the doc_id, and the score \n",
    "        current_topic_id, _, doc_id, rank, score, _ = line.strip().split()\n",
    "\n",
    "        # If the current topic is different from the previous line\n",
    "        if old_topic_id != current_topic_id:\n",
    "\n",
    "            # If this is the first iteration\n",
    "            if old_topic_id == \"-\":\n",
    "\n",
    "                # Per topic ranking, list of tuples (doc_id, scores)\n",
    "                # Intitialized as empty\n",
    "                ranking = []\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Sort the ranking as in trec_eval\n",
    "                ranking.sort(key=itemgetter(0), reverse=True)         # doc_id descending\n",
    "                ranking.sort(key=itemgetter(1), reverse=True)         # score descending\n",
    "\n",
    "                # Save the ranking in the dictionary\n",
    "                # We need just the doci_id\n",
    "                run[old_topic_id] = [x for (x, y) in ranking]\n",
    "\n",
    "                # Intitialized as empty\n",
    "                ranking = []\n",
    "\n",
    "        # Add the tuple to the list\n",
    "        ranking.append((doc_id, float(score)))\n",
    "\n",
    "        # Update the topic_id for the next iteration\n",
    "        old_topic_id = current_topic_id\n",
    "\n",
    "    # We still need to add the ranking for the last topic\n",
    "    ranking.sort(key=itemgetter(0), reverse=True)         # doc_id descending\n",
    "    ranking.sort(key=itemgetter(1), reverse=True)         # score descending\n",
    "    run[current_topic_id] = [x for (x, y) in ranking]\n",
    "    \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(rbo_results, topic_ids, runsfile_names, output_path, original_run_name, measure_name, p):\n",
    "    # Create the csv file and write the measures scores\n",
    "    \n",
    "    # Args:\n",
    "        # rbo_results: dictionary with rbo scores for each run and topic\n",
    "        # topic_ids: topic idssorted in ascending way\n",
    "        # runsfile_names: run ids sorted in ascending way\n",
    "        # output_path: folder which will contain the output results\n",
    "        # original_run_name: original run name to be used to defune the result file name\n",
    "        # measure_name: RBO with cutoff to be used to defune the result file name\n",
    "        # p: RBO persistency parameter to be used to defune the result file name\n",
    "\n",
    "    # Define the file name\n",
    "    output_file_path = os.path.join(output_path, \"rpl_\" + original_run_name.lower() + \"_\" + measure_name + \"_p\" + str(p).replace(\".\", \"\") +\".csv\")\n",
    "    # Create the result file\n",
    "    with open(output_file_path, \"w\") as f:\n",
    "        # Create the header: run_id and topic ids\n",
    "        header = \"topic_id,\" + \",\".join(runsfile_names) + \"\\n\"\n",
    "        # write the header in the file\n",
    "        f.write(header)\n",
    "\n",
    "        # Loop over the topics\n",
    "        for topic_id in topic_ids:\n",
    "            # Empty list which will store the score for each topic\n",
    "            rbo_scores = []\n",
    "            # Loop over the runs\n",
    "            for run_id in runsfile_names:\n",
    "                # Get the measures score\n",
    "                rbo_scores.append(str(rbo_results.get((run_id, topic_id))))\n",
    "\n",
    "            # Create the line which will be written in the csv file\n",
    "            result_line =  topic_id + \",\" + (\",\".join(rbo_scores)) + \"\\n\"\n",
    "            # Write the line in the file\n",
    "            f.write(result_line)\n",
    "\n",
    "        # Close the file\n",
    "        f.close()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import the Original Run</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the original run\n",
    "original_run = import_run(original_run_file)\n",
    "\n",
    "# Get the list of topic_id\n",
    "topic_ids = original_run.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths of the replicability runs\n",
    "# List all the files in the subfolders\n",
    "# List of tuples: (run_path, run_id) \n",
    "run_files = []\n",
    "# Walk through the subfolders\n",
    "for path, subdirs, files in os.walk(replicability_runs_folder):\n",
    "    # For each file in the subfolders\n",
    "    for name in files:\n",
    "        # Check if this is a run file, the extension should be .txt\n",
    "        if name.endswith(\".txt\"):\n",
    "            # Append the tuple to the list\n",
    "            # os.path.join(path, name) returns the path to the run file\n",
    "            # name.replace(\".txt\", \"\") remove the extension\n",
    "            run_files.append((os.path.join(path, name), name.replace(\".txt\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are saved in a dictionary\n",
    "# key: (run_id, topic_id)\n",
    "# value: RBO score for each topic, float\n",
    "# RBO is computed at cut-off 10, 100, and 1000\n",
    "results_rbo_5 = {}\n",
    "results_rbo_10 = {}\n",
    "results_rbo_20 = {}\n",
    "results_rbo_50 = {}\n",
    "results_rbo_100 = {}\n",
    "results_rbo_1000 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the runs on the replicability folder\n",
    "# For each run compute RBO between the original and the replicated run\n",
    "for (run_path, run_name) in run_files:\n",
    "    \n",
    "    # Import the run\n",
    "    replicated_run = import_run(run_path)\n",
    "    \n",
    "    # For each topic\n",
    "    for topic_id in topic_ids:\n",
    "        \n",
    "        # get the corresponding original and replicated ranking\n",
    "        original_ranking = original_run[topic_id]\n",
    "        replicated_ranking = replicated_run[topic_id]\n",
    "        # Compute RBO for each cut-off\n",
    "        rbo_5 = rbo.rbo(original_ranking[:4], replicated_ranking[:4], p)\n",
    "        #rbo_10 = rbo.rbo(original_ranking[:9], replicated_ranking[:9], p)\n",
    "        rbo_20 = rbo.rbo(original_ranking[:19], replicated_ranking[:19], p)\n",
    "        rbo_50 = rbo.rbo(original_ranking[:49], replicated_ranking[:49], p)\n",
    "        #rbo_100 = rbo.rbo(original_ranking[:99], replicated_ranking[:99], p)\n",
    "        #rbo_1000 = rbo.rbo(original_ranking[:999], replicated_ranking[:999], p)\n",
    "    \n",
    "        # Store the result in the dictionary\n",
    "        results_rbo_5[(run_name, topic_id)] = rbo_5[\"ext\"]\n",
    "        #results_rbo_10[(run_name, topic_id)] = rbo_10[\"ext\"]\n",
    "        results_rbo_20[(run_name, topic_id)] = rbo_20[\"ext\"]\n",
    "        results_rbo_50[(run_name, topic_id)] = rbo_50[\"ext\"]\n",
    "        #results_rbo_100[(run_name, topic_id)] = rbo_100[\"ext\"]\n",
    "        #results_rbo_1000[(run_name, topic_id)] = rbo_1000[\"ext\"]\n",
    "    \n",
    "    \n",
    "    # Delete the imported run\n",
    "    replicated_run.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Print the results in a CSV file</h3>\n",
    "<p>This will generate the tables for the SIGIR paper.\n",
    "The csv is formatted as follows:<br>\n",
    "<code>run_id,score_t1,score_t2,...,score_tN</code><br>\n",
    "The scores use the dot as decimal separator.<br>\n",
    "For each measure, we generate a different file.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RBO results in a CSV file\n",
    "# Sorted list of topics\n",
    "topic_ids = sorted(topic_ids)\n",
    "# topic_ids.sort()\n",
    "output_path = os.path.join(\"/Users/tjz514/Documents/uni/2020/sigir2020_reproducibility/experiments/matlab/results/measures\")\n",
    "\n",
    "# Sort the runs alphabetically\n",
    "runsfile_names = [y for (x, y) in run_files]\n",
    "runsfile_names.sort()\n",
    "\n",
    "# Print the results\n",
    "print_results(results_rbo_5, topic_ids, runsfile_names, output_path, original_run_name, \"rbo_5\", p)\n",
    "#print_results(results_rbo_10, topic_ids, runsfile_names, output_path, original_run_name, \"rbo_10\", p)\n",
    "print_results(results_rbo_20, topic_ids, runsfile_names, output_path, original_run_name, \"rbo_20\", p)\n",
    "print_results(results_rbo_50, topic_ids, runsfile_names, output_path, original_run_name, \"rbo_50\", p)\n",
    "#print_results(results_rbo_100, topic_ids, runsfile_names, output_path, original_run_name, \"rbo_100\", p)\n",
    "#print_results(results_rbo_1000, topic_ids, runsfile_names, output_path, original_run_name, \"rbo_1000\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
